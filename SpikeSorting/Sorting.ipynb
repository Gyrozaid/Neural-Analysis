{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CPWI18_2019-08-01_08-47-01_RLLRRL 420uA': '/Volumes/Lab/Ephys/RawData/CPWI18_2019-08-01_08-47-01_RLLRRL 420uA', 'CPWI18_2019-07-19_13-21-54_RLLRRL 400mA': '/Volumes/Lab/Ephys/RawData/CPWI18_2019-07-19_13-21-54_RLLRRL 400mA', 'CPWI18_2019-07-29_13-50-50_LRRLLR 400uA': '/Volumes/Lab/Ephys/RawData/CPWI18_2019-07-29_13-50-50_LRRLLR 400uA'}\n"
     ]
    }
   ],
   "source": [
    "import shutil \n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "#pathToData = \"/mnt/Ephys/NAc_Ephys_raw_data/USB_Copy_2022-12-09_120822\"\n",
    "pathToData = \"/Volumes/Lab/Ephys/RawData\"\n",
    "\n",
    "#Determine each animal folder path and extract the actual name for the animals folder\n",
    "recFolders = {}\n",
    "for file in os.listdir(path=pathToData):\n",
    "    fileStr = str(file)\n",
    "    if fileStr[:3].lower() == \"cpw\":\n",
    "        fileLoc = Path(f\"{pathToData}/{file}\")\n",
    "        recFolders[fileStr] = str(fileLoc)\n",
    "    #elif fileStr[:8].lower() != \"cpwi17_s\":\n",
    "    #    for file2 in os.listdir(path=f\"{pathToData}/{file}\"):\n",
    "    #        fileLoc2 = Path(f\"{pathToData}/{file}/{file2}\")\n",
    "    #        folders.append(fileLoc2)\n",
    "    #        folderNames.append(str(file2))\n",
    "print(recFolders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfolders = [\\n    #Path('/mnt/Ephys/NAc_Ephys_raw_data/USB_Copy_2022-12-09_120822/CPWI17_2019-09-25_11-07-37_260uA RLLRRL'),\\n    #Path('/mnt/Ephys/NAc_Ephys_raw_data/USB_Copy_2022-12-09_120822/CPWI17_2019-10-03_11-59-29_260uA RLLRRL'),\\n    #Path('/mnt/Ephys/NAc_Ephys_raw_data/USB_Copy_2022-12-09_120822/CPWI17_2019-10-04_09-46-35_260uA LRRLLR'),\\n    #Path('/mnt/Ephys/NAc_Ephys_raw_data/USB_Copy_2022-12-09_120822/CPWI17_2019-10-29_14-57-42_260uA RLLRRL'),\\n    #Path('/mnt/Ephys/NAc_Ephys_raw_data/USB_Copy_2022-12-09_120822/CPWI18_2019-07-02_14-02-15_RLLRRL 30mA'),\\n    Path('/mnt/Ephys/NAc_Ephys_raw_data/USB_Copy_2022-12-09_120822/CPWI1902_2020-01-16_09-26-04_490uA LRRLLR')\\n           ]\\nfolderNames = [\\n    #'CPWI17_2019-10-29_14-57-42_260uA RLLRRL'\\n    'CPWI1902_2020-01-16_09-26-04_490uA LRRLLR'\\n]\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "folders = [\n",
    "    #Path('/mnt/Ephys/NAc_Ephys_raw_data/USB_Copy_2022-12-09_120822/CPWI17_2019-09-25_11-07-37_260uA RLLRRL'),\n",
    "    #Path('/mnt/Ephys/NAc_Ephys_raw_data/USB_Copy_2022-12-09_120822/CPWI17_2019-10-03_11-59-29_260uA RLLRRL'),\n",
    "    #Path('/mnt/Ephys/NAc_Ephys_raw_data/USB_Copy_2022-12-09_120822/CPWI17_2019-10-04_09-46-35_260uA LRRLLR'),\n",
    "    #Path('/mnt/Ephys/NAc_Ephys_raw_data/USB_Copy_2022-12-09_120822/CPWI17_2019-10-29_14-57-42_260uA RLLRRL'),\n",
    "    #Path('/mnt/Ephys/NAc_Ephys_raw_data/USB_Copy_2022-12-09_120822/CPWI18_2019-07-02_14-02-15_RLLRRL 30mA'),\n",
    "    Path('/mnt/Ephys/NAc_Ephys_raw_data/USB_Copy_2022-12-09_120822/CPWI1902_2020-01-16_09-26-04_490uA LRRLLR')\n",
    "           ]\n",
    "folderNames = [\n",
    "    #'CPWI17_2019-10-29_14-57-42_260uA RLLRRL'\n",
    "    'CPWI1902_2020-01-16_09-26-04_490uA LRRLLR'\n",
    "]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Source: /Volumes/Lab/Ephys/RawData/CPWI18_2019-08-01_08-47-01_RLLRRL 420uA\n",
      "stream_id 1 is not in [np.str_('0')]\n",
      "Tried 1\n",
      "BandpassFilterRecording: 32 channels - 30.0kHz - 1 segments - 239,508,300 samples \n",
      "                         7,983.61s (2.22 hours) - int16 dtype - 14.28 GiB\n",
      "[[ 0  1]\n",
      " [ 0  2]\n",
      " [ 0  3]\n",
      " [ 0  4]\n",
      " [ 0  5]\n",
      " [ 0  6]\n",
      " [ 0  7]\n",
      " [ 0  8]\n",
      " [ 0  9]\n",
      " [ 0 10]\n",
      " [ 0 11]\n",
      " [ 0 12]\n",
      " [ 0 13]\n",
      " [ 0 14]\n",
      " [ 0 15]\n",
      " [ 0 16]\n",
      " [ 0 17]\n",
      " [ 0 18]\n",
      " [ 0 19]\n",
      " [ 0 20]\n",
      " [ 0 21]\n",
      " [ 0 22]\n",
      " [ 0 23]\n",
      " [ 0 24]\n",
      " [ 0 25]\n",
      " [ 0 26]\n",
      " [ 0 27]\n",
      " [ 0 28]\n",
      " [ 0 29]\n",
      " [ 0 30]\n",
      " [ 0 31]\n",
      " [ 0 32]]\n",
      "ChannelSliceRecording: 32 channels - 30.0kHz - 1 segments - 239,508,300 samples \n",
      "                       7,983.61s (2.22 hours) - int16 dtype - 14.28 GiB\n",
      "['CH4' 'CH5' 'CH6' 'CH7' 'CH2' 'CH3' 'CH30' 'CH31' 'CH26' 'CH27' 'CH28'\n",
      " 'CH29' 'CH22' 'CH23' 'CH24' 'CH25' 'CH18' 'CH19' 'CH20' 'CH21' 'CH1'\n",
      " 'CH16' 'CH17' 'CH32' 'CH12' 'CH13' 'CH14' 'CH15' 'CH8' 'CH9' 'CH10'\n",
      " 'CH11']\n",
      "    probe_index             x             y contact_shapes  radius shank_ids  \\\n",
      "0             0  1.000000e+01  0.000000e+00         circle     6.0             \n",
      "1             0  6.123234e-16  1.000000e+01         circle     6.0             \n",
      "2             0 -1.000000e+01  1.224647e-15         circle     6.0             \n",
      "3             0 -1.836970e-15 -1.000000e+01         circle     6.0             \n",
      "4             1  6.000000e+01  0.000000e+00         circle     6.0             \n",
      "5             1  5.000000e+01  1.000000e+01         circle     6.0             \n",
      "6             1  4.000000e+01  1.224647e-15         circle     6.0             \n",
      "7             1  5.000000e+01 -1.000000e+01         circle     6.0             \n",
      "8             2  1.100000e+02  0.000000e+00         circle     6.0             \n",
      "9             2  1.000000e+02  1.000000e+01         circle     6.0             \n",
      "10            2  9.000000e+01  1.224647e-15         circle     6.0             \n",
      "11            2  1.000000e+02 -1.000000e+01         circle     6.0             \n",
      "12            3  1.600000e+02  0.000000e+00         circle     6.0             \n",
      "13            3  1.500000e+02  1.000000e+01         circle     6.0             \n",
      "14            3  1.400000e+02  1.224647e-15         circle     6.0             \n",
      "15            3  1.500000e+02 -1.000000e+01         circle     6.0             \n",
      "16            4  2.100000e+02  0.000000e+00         circle     6.0             \n",
      "17            4  2.000000e+02  1.000000e+01         circle     6.0             \n",
      "18            4  1.900000e+02  1.224647e-15         circle     6.0             \n",
      "19            4  2.000000e+02 -1.000000e+01         circle     6.0             \n",
      "20            5  2.600000e+02  0.000000e+00         circle     6.0             \n",
      "21            5  2.500000e+02  1.000000e+01         circle     6.0             \n",
      "22            5  2.400000e+02  1.224647e-15         circle     6.0             \n",
      "23            5  2.500000e+02 -1.000000e+01         circle     6.0             \n",
      "24            6  3.100000e+02  0.000000e+00         circle     6.0             \n",
      "25            6  3.000000e+02  1.000000e+01         circle     6.0             \n",
      "26            6  2.900000e+02  1.224647e-15         circle     6.0             \n",
      "27            6  3.000000e+02 -1.000000e+01         circle     6.0             \n",
      "28            7  3.600000e+02  0.000000e+00         circle     6.0             \n",
      "29            7  3.500000e+02  1.000000e+01         circle     6.0             \n",
      "30            7  3.400000e+02  1.224647e-15         circle     6.0             \n",
      "31            7  3.500000e+02 -1.000000e+01         circle     6.0             \n",
      "\n",
      "   contact_ids  \n",
      "0               \n",
      "1               \n",
      "2               \n",
      "3               \n",
      "4               \n",
      "5               \n",
      "6               \n",
      "7               \n",
      "8               \n",
      "9               \n",
      "10              \n",
      "11              \n",
      "12              \n",
      "13              \n",
      "14              \n",
      "15              \n",
      "16              \n",
      "17              \n",
      "18              \n",
      "19              \n",
      "20              \n",
      "21              \n",
      "22              \n",
      "23              \n",
      "24              \n",
      "25              \n",
      "26              \n",
      "27              \n",
      "28              \n",
      "29              \n",
      "30              \n",
      "31              \n",
      "Channel Groups: [5 1 1 0 0 0 0 7 7 7 7 6 6 6 6 5 5 4 4 4 4 3 3 3 3 2 2 2 2 1 1 5]\n",
      "Channel IDs: ['CH4' 'CH5' 'CH6' 'CH7' 'CH2' 'CH3' 'CH30' 'CH31' 'CH26' 'CH27' 'CH28'\n",
      " 'CH29' 'CH22' 'CH23' 'CH24' 'CH25' 'CH18' 'CH19' 'CH20' 'CH21' 'CH1'\n",
      " 'CH16' 'CH17' 'CH32' 'CH12' 'CH13' 'CH14' 'CH15' 'CH8' 'CH9' 'CH10'\n",
      " 'CH11']\n",
      "Channel Locations: [[ 1.0000000e+01  0.0000000e+00]\n",
      " [ 6.1232340e-16  1.0000000e+01]\n",
      " [-1.0000000e+01  1.2246468e-15]\n",
      " [-1.8369702e-15 -1.0000000e+01]\n",
      " [ 6.0000000e+01  0.0000000e+00]\n",
      " [ 5.0000000e+01  1.0000000e+01]\n",
      " [ 4.0000000e+01  1.2246468e-15]\n",
      " [ 5.0000000e+01 -1.0000000e+01]\n",
      " [ 1.1000000e+02  0.0000000e+00]\n",
      " [ 1.0000000e+02  1.0000000e+01]\n",
      " [ 9.0000000e+01  1.2246468e-15]\n",
      " [ 1.0000000e+02 -1.0000000e+01]\n",
      " [ 1.6000000e+02  0.0000000e+00]\n",
      " [ 1.5000000e+02  1.0000000e+01]\n",
      " [ 1.4000000e+02  1.2246468e-15]\n",
      " [ 1.5000000e+02 -1.0000000e+01]\n",
      " [ 2.1000000e+02  0.0000000e+00]\n",
      " [ 2.0000000e+02  1.0000000e+01]\n",
      " [ 1.9000000e+02  1.2246468e-15]\n",
      " [ 2.0000000e+02 -1.0000000e+01]\n",
      " [ 2.6000000e+02  0.0000000e+00]\n",
      " [ 2.5000000e+02  1.0000000e+01]\n",
      " [ 2.4000000e+02  1.2246468e-15]\n",
      " [ 2.5000000e+02 -1.0000000e+01]\n",
      " [ 3.1000000e+02  0.0000000e+00]\n",
      " [ 3.0000000e+02  1.0000000e+01]\n",
      " [ 2.9000000e+02  1.2246468e-15]\n",
      " [ 3.0000000e+02 -1.0000000e+01]\n",
      " [ 3.6000000e+02  0.0000000e+00]\n",
      " [ 3.5000000e+02  1.0000000e+01]\n",
      " [ 3.4000000e+02  1.2246468e-15]\n",
      " [ 3.5000000e+02 -1.0000000e+01]]\n",
      "write_binary_recording \n",
      "n_jobs=8 - samples_per_chunk=30,000 - chunk_memory=468.75 KiB - total_memory=3.66 MiB - chunk_duration=1.00s\n",
      "\n",
      "=============================================\n",
      "Processing block 1 of 4...\n",
      "Using training recording of duration 300 sec with the sampling mode uniform\n",
      "*** MS5 Elapsed time for SCHEME2 get_sampled_recording_for_training: 0.351 seconds ***\n",
      "Running phase 1 sorting\n",
      "Number of channels: 4\n",
      "Number of timepoints: 9000000\n",
      "Sampling frequency: 30000.0 Hz\n",
      "Channel 0: [10.  0.]\n",
      "Channel 1: [6.123234e-16 1.000000e+01]\n",
      "Channel 2: [-1.0000000e+01  1.2246468e-15]\n",
      "Channel 3: [-1.8369702e-15 -1.0000000e+01]\n",
      "Loading traces\n",
      "*** MS5 Elapsed time for load_traces: 0.000 seconds ***\n",
      "Detecting spikes\n",
      "\n",
      "Adjacency for detect spikes with channel radius 200\n",
      "[[0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3]]\n",
      "\n",
      "m = 0 (nbhd size: 4)\n",
      "m = 1 (nbhd size: 4)\n",
      "m = 2 (nbhd size: 4)\n",
      "m = 3 (nbhd size: 4)\n",
      "Detected 11248 spikes\n",
      "*** MS5 Elapsed time for detect_spikes: 0.185 seconds ***\n",
      "Removing duplicate times\n",
      "*** MS5 Elapsed time for remove_duplicate_times: 0.000 seconds ***\n",
      "Extracting 11248 snippets\n",
      "*** MS5 Elapsed time for extract_snippets: 0.034 seconds ***\n",
      "Computing PCA features with npca=12\n",
      "*** MS5 Elapsed time for compute_pca_features: 0.014 seconds ***\n",
      "Isosplit6 clustering with npca_per_subdivision=10\n",
      "Found 3 clusters\n",
      "*** MS5 Elapsed time for isosplit6_subdivision_method: 0.251 seconds ***\n",
      "Computing templates\n",
      "*** MS5 Elapsed time for compute_templates: 0.023 seconds ***\n",
      "Determining optimal alignment of templates\n"
     ]
    },
    {
     "ename": "SpikeSortingError",
     "evalue": "Spike sorting error trace:\nTraceback (most recent call last):\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/spikesorting/lib/python3.10/site-packages/spikeinterface/sorters/basesorter.py\", line 261, in run_from_folder\n    SorterClass._run_from_folder(sorter_output_folder, sorter_params, verbose)\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/spikesorting/lib/python3.10/site-packages/spikeinterface/sorters/external/mountainsort5.py\", line 197, in _run_from_folder\n    sorting = ms5.sorting_scheme3(recording=recording_cached, sorting_parameters=scheme3_sorting_parameters)\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/spikesorting/lib/python3.10/site-packages/mountainsort5/schemes/sorting_scheme3.py\", line 57, in sorting_scheme3\n    result = sorting_scheme2(\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/spikesorting/lib/python3.10/site-packages/mountainsort5/schemes/sorting_scheme2.py\", line 87, in sorting_scheme2\n    sorting1 = sorting_scheme1(\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/spikesorting/lib/python3.10/site-packages/mountainsort5/schemes/sorting_scheme1.py\", line 135, in sorting_scheme1\n    offsets = align_templates(templates)\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/spikesorting/lib/python3.10/site-packages/mountainsort5/schemes/sorting_scheme1.py\", line 244, in align_templates\n    offset, inner_product = compute_pairwise_optimal_offset(templates[k1], templates[k2])\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/spikesorting/lib/python3.10/site-packages/mountainsort5/schemes/sorting_scheme1.py\", line 274, in compute_pairwise_optimal_offset\n    best_inner_product = -np.Inf\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/spikesorting/lib/python3.10/site-packages/numpy/__init__.py\", line 397, in __getattr__\n    raise AttributeError(\nAttributeError: `np.Inf` was removed in the NumPy 2.0 release. Use `np.inf` instead.. Did you mean: 'inf'?\n\nSpike sorting failed. You can inspect the runtime trace in /Volumes/Lab/Ephys/EphysOutputs/OutputCPWI18_2019-08-01_08-47-01_RLLRRL 420uA/SortMS5/0/spikeinterface_log.json.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSpikeSortingError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 251\u001b[0m\n\u001b[1;32m    249\u001b[0m fullDir \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdirectory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Path\u001b[38;5;241m.\u001b[39mis_dir(fullDir):\n\u001b[0;32m--> 251\u001b[0m     sortingMS5 \u001b[38;5;241m=\u001b[39m \u001b[43msorters\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_sorter_by_property\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43msorter_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmountainsort5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrecording\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrecordingCom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrouping_property\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgroup\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfolder\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfullDir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/SortMS5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#sorter parameters\u001b[39;49;00m\n\u001b[1;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscheme\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m3\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#detect_threshold = thresholdList[i]\u001b[39;49;00m\n\u001b[1;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#detect_sign = -1,\u001b[39;49;00m\n\u001b[1;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#detect_time_radius_msec =  0.5,\u001b[39;49;00m\n\u001b[1;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#snippet_T1 = 20,\u001b[39;49;00m\n\u001b[1;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#snippet_T2 = 20,\u001b[39;49;00m\n\u001b[1;32m    263\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#npca_per_channel = 3,\u001b[39;49;00m\n\u001b[1;32m    264\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#npca_per_subdivision = 10,\u001b[39;49;00m\n\u001b[1;32m    265\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#snippet_mask_radius = 250,\u001b[39;49;00m\n\u001b[1;32m    266\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#scheme1_detect_channel_radius = 150,\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#scheme2_phase1_detect_channel_radius = 200,\u001b[39;49;00m\n\u001b[1;32m    268\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#scheme2_detect_channel_radius = 50,\u001b[39;49;00m\n\u001b[1;32m    269\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#scheme2_max_num_snippets_per_training_batch = 200,\u001b[39;49;00m\n\u001b[1;32m    270\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#scheme2_training_duration_sec = 300,\u001b[39;49;00m\n\u001b[1;32m    271\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#scheme2_training_recording_sampling_mode = 'uniform',\u001b[39;49;00m\n\u001b[1;32m    272\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#scheme3_block_duration_sec = 1800,\u001b[39;49;00m\n\u001b[1;32m    273\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#freq_min = 300,\u001b[39;49;00m\n\u001b[1;32m    274\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#freq_max = 6000,\u001b[39;49;00m\n\u001b[1;32m    275\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#filter = True,\u001b[39;49;00m\n\u001b[1;32m    276\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#whiten = True\u001b[39;49;00m\n\u001b[1;32m    277\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    278\u001b[0m     toAddMS5 \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMS5\u001b[39m\u001b[38;5;124m'\u001b[39m,sortingMS5]\n\u001b[1;32m    279\u001b[0m     sortingObjects\u001b[38;5;241m.\u001b[39mappend(toAddMS5)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/spikesorting/lib/python3.10/site-packages/spikeinterface/sorters/launcher.py:306\u001b[0m, in \u001b[0;36mrun_sorter_by_property\u001b[0;34m(sorter_name, recording, grouping_property, folder, mode_if_folder_exists, engine, engine_kwargs, verbose, docker_image, singularity_image, working_folder, **sorter_params)\u001b[0m\n\u001b[1;32m    295\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m    296\u001b[0m         sorter_name\u001b[38;5;241m=\u001b[39msorter_name,\n\u001b[1;32m    297\u001b[0m         recording\u001b[38;5;241m=\u001b[39mrec,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msorter_params,\n\u001b[1;32m    303\u001b[0m     )\n\u001b[1;32m    304\u001b[0m     job_list\u001b[38;5;241m.\u001b[39mappend(job)\n\u001b[0;32m--> 306\u001b[0m sorting_list \u001b[38;5;241m=\u001b[39m \u001b[43mrun_sorter_jobs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjob_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    308\u001b[0m unit_groups \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sorting, group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sorting_list, recording_dict\u001b[38;5;241m.\u001b[39mkeys()):\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/spikesorting/lib/python3.10/site-packages/spikeinterface/sorters/launcher.py:106\u001b[0m, in \u001b[0;36mrun_sorter_jobs\u001b[0;34m(job_list, engine, engine_kwargs, return_output)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m engine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloop\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;66;03m# simple loop in main process\u001b[39;00m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m kwargs \u001b[38;5;129;01min\u001b[39;00m job_list:\n\u001b[0;32m--> 106\u001b[0m         sorting \u001b[38;5;241m=\u001b[39m \u001b[43mrun_sorter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m return_output:\n\u001b[1;32m    108\u001b[0m             out\u001b[38;5;241m.\u001b[39mappend(sorting)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/spikesorting/lib/python3.10/site-packages/spikeinterface/sorters/runsorter.py:216\u001b[0m, in \u001b[0;36mrun_sorter\u001b[0;34m(sorter_name, recording, folder, remove_existing_folder, delete_output_folder, verbose, raise_error, docker_image, singularity_image, delete_container_files, with_output, output_folder, **sorter_params)\u001b[0m\n\u001b[1;32m    205\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    206\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe python `spython` package must be installed to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    207\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun singularity. Install with `pip install spython`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    208\u001b[0m             )\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m run_sorter_container(\n\u001b[1;32m    211\u001b[0m         container_image\u001b[38;5;241m=\u001b[39mcontainer_image,\n\u001b[1;32m    212\u001b[0m         mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[1;32m    213\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcommon_kwargs,\n\u001b[1;32m    214\u001b[0m     )\n\u001b[0;32m--> 216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun_sorter_local\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcommon_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/spikesorting/lib/python3.10/site-packages/spikeinterface/sorters/runsorter.py:276\u001b[0m, in \u001b[0;36mrun_sorter_local\u001b[0;34m(sorter_name, recording, folder, remove_existing_folder, delete_output_folder, verbose, raise_error, with_output, output_folder, **sorter_params)\u001b[0m\n\u001b[1;32m    274\u001b[0m SorterClass\u001b[38;5;241m.\u001b[39mset_params_to_folder(recording, folder, sorter_params, verbose)\n\u001b[1;32m    275\u001b[0m SorterClass\u001b[38;5;241m.\u001b[39msetup_recording(recording, folder, verbose\u001b[38;5;241m=\u001b[39mverbose)\n\u001b[0;32m--> 276\u001b[0m \u001b[43mSorterClass\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_from_folder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraise_error\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_output:\n\u001b[1;32m    278\u001b[0m     sorting \u001b[38;5;241m=\u001b[39m SorterClass\u001b[38;5;241m.\u001b[39mget_result_from_folder(folder, register_recording\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, sorting_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/spikesorting/lib/python3.10/site-packages/spikeinterface/sorters/basesorter.py:301\u001b[0m, in \u001b[0;36mBaseSorter.run_from_folder\u001b[0;34m(cls, output_folder, raise_error, verbose)\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msorter_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m run time \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m0.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_error \u001b[38;5;129;01mand\u001b[39;00m raise_error:\n\u001b[0;32m--> 301\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SpikeSortingError(\n\u001b[1;32m    302\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpike sorting error trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merror_log_to_display\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    303\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpike sorting failed. You can inspect the runtime trace in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_folder\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/spikeinterface_log.json.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    304\u001b[0m     )\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m run_time\n",
      "\u001b[0;31mSpikeSortingError\u001b[0m: Spike sorting error trace:\nTraceback (most recent call last):\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/spikesorting/lib/python3.10/site-packages/spikeinterface/sorters/basesorter.py\", line 261, in run_from_folder\n    SorterClass._run_from_folder(sorter_output_folder, sorter_params, verbose)\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/spikesorting/lib/python3.10/site-packages/spikeinterface/sorters/external/mountainsort5.py\", line 197, in _run_from_folder\n    sorting = ms5.sorting_scheme3(recording=recording_cached, sorting_parameters=scheme3_sorting_parameters)\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/spikesorting/lib/python3.10/site-packages/mountainsort5/schemes/sorting_scheme3.py\", line 57, in sorting_scheme3\n    result = sorting_scheme2(\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/spikesorting/lib/python3.10/site-packages/mountainsort5/schemes/sorting_scheme2.py\", line 87, in sorting_scheme2\n    sorting1 = sorting_scheme1(\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/spikesorting/lib/python3.10/site-packages/mountainsort5/schemes/sorting_scheme1.py\", line 135, in sorting_scheme1\n    offsets = align_templates(templates)\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/spikesorting/lib/python3.10/site-packages/mountainsort5/schemes/sorting_scheme1.py\", line 244, in align_templates\n    offset, inner_product = compute_pairwise_optimal_offset(templates[k1], templates[k2])\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/spikesorting/lib/python3.10/site-packages/mountainsort5/schemes/sorting_scheme1.py\", line 274, in compute_pairwise_optimal_offset\n    best_inner_product = -np.Inf\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/spikesorting/lib/python3.10/site-packages/numpy/__init__.py\", line 397, in __getattr__\n    raise AttributeError(\nAttributeError: `np.Inf` was removed in the NumPy 2.0 release. Use `np.inf` instead.. Did you mean: 'inf'?\n\nSpike sorting failed. You can inspect the runtime trace in /Volumes/Lab/Ephys/EphysOutputs/OutputCPWI18_2019-08-01_08-47-01_RLLRRL 420uA/SortMS5/0/spikeinterface_log.json."
     ]
    }
   ],
   "source": [
    "import spikeinterface.full as si\n",
    "from pathlib import Path\n",
    "global_job_kwargs = dict(n_jobs=-1, chunk_duration=\"1s\",progress_bar=True)\n",
    "si.set_global_job_kwargs(**global_job_kwargs)\n",
    "import spikeinterface.extractors as se\n",
    "import spikeinterface.preprocessing as pp\n",
    "\n",
    "\n",
    "from probeinterface import Probe, ProbeGroup, generate_tetrode\n",
    "from probeinterface.plotting import plot_probe_group\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from spikeinterface import sorters\n",
    "\n",
    "from spikeinterface.postprocessing import compute_spike_amplitudes, compute_correlograms, compute_template_similarity\n",
    "from spikeinterface.qualitymetrics import compute_quality_metrics, compute_num_spikes\n",
    "from spikeinterface.exporters import export_report, export_to_phy\n",
    "import spikeinterface.qualitymetrics as qual\n",
    "from spikeinterface import create_sorting_analyzer\n",
    "\n",
    "from spikeinterface.postprocessing import compute_principal_components\n",
    "\n",
    "import spikeinterface.postprocessing as spost\n",
    "\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "\n",
    "import pickle\n",
    "\n",
    "import math\n",
    "\n",
    "#Create log file to keep track of erros in pipeline\n",
    "import logging\n",
    "now = datetime.now()\n",
    "logString = now.strftime(\"%d%m%Y %H%M%S\")\n",
    "logging.basicConfig(filename=f\"logs/{logString}run.log\", force=True)\n",
    "process = True\n",
    "\n",
    "sortingObjects = []\n",
    "#Iterate over every animal folder selected in first cell\n",
    "for recordingName, recordingLoc in recFolders.items():\n",
    "    pipelineBroke = False\n",
    "    probeMismatch = False\n",
    "    correctChannels = True\n",
    "    recName = recordingName\n",
    "    #Change these as desired\n",
    "    finalDir = Path(f\"/Volumes/Lab/Ephys/SortingOutputs/Output{recName}\")\n",
    "    directory = Path(f\"/Volumes/Lab/Ephys/EphysOutputs/Output{recName}\")\n",
    "    dataSource = recordingLoc\n",
    "    print(f\"Data Source: {dataSource}\")\n",
    "    #dataCopyLoc = Path(f\"/home/moormanlab/Documents/CPWI17/{recName}\")\n",
    "    numberOfOrigChannels = 0\n",
    "    \n",
    "    #Don't sort if already sorted\n",
    "    #if (not Path.is_dir(dataCopyLoc)) and (not Path.is_dir(finalDir)):\n",
    "    if True:\n",
    "        #print(\"Copying\")\n",
    "        #shutil.copytree(\n",
    "        #    src=dataSource,\n",
    "        #    dst=dataCopyLoc\n",
    "        #    )\n",
    "        #print(\"Done Copying\")\n",
    "        totalExperiments = len(next(os.walk(dataSource))[1])\n",
    "        blockIndex = int(totalExperiments) - 1\n",
    "        try:\n",
    "            dataLocal = se.read_openephys(folder_path=dataSource, stream_id = '1', block_index = blockIndex)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"Tried 1\")\n",
    "            try:\n",
    "                dataLocal = se.read_openephys(folder_path=dataSource, stream_id = '0', block_index = blockIndex)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(\"Tried 0\")\n",
    "                continue\n",
    "        #Generic bandpass filter\n",
    "        recordingObject = pp.bandpass_filter(dataLocal, freq_min=300, freq_max=6000)\n",
    "        numberOfOrigChannels = int(recordingObject.get_num_channels())\n",
    "        if (numberOfOrigChannels == 64) or (numberOfOrigChannels == 67):\n",
    "            correctChannels = True\n",
    "            #channel_ids = np.array(channel_ids)\n",
    "            print(recordingObject)\n",
    "            '''\n",
    "            try:\n",
    "                numChannels = 67\n",
    "                x = [0] * numChannels\n",
    "                y = list(range(1, numChannels+1))\n",
    "                locations = list(zip(x,y))\n",
    "                locations = np.array(locations)\n",
    "                locations.shape\n",
    "                origChanIDs = recordingObject.get_channel_ids().tolist()\n",
    "                auxChannels = ['AUX1','AUX2','AUX3','aux1','aux2','aux3']\n",
    "                auxChannels2 = ['AUX1','AUX2','AUX3']\n",
    "                newChanIDs = [channel for channel in origChanIDs if channel not in auxChannels]\n",
    "                newChanIDs = np.asarray(newChanIDs)\n",
    "                recordingObject.set_channel_locations(locations,origChanIDs)\n",
    "                recordingObject = recordingObject.remove_channels(auxChannels2)\n",
    "            '''\n",
    "            #Remove AUX channels\n",
    "            origChanIDs = recordingObject.get_channel_ids().tolist()\n",
    "            process = True\n",
    "            auxChannels = ['AUX1','AUX2','AUX3','aux1','aux2','aux3']\n",
    "            newChanIDs = [channel for channel in origChanIDs if channel not in auxChannels]\n",
    "            \n",
    "            #Old code block to remove excess channels for odd numbers\n",
    "            numChannels = len(newChanIDs)\n",
    "            totalTetrodeChannels = int(4 * math.floor(numChannels/4))\n",
    "            channelsToCut = newChanIDs[:numChannels-totalTetrodeChannels]\n",
    "            #newChanIDs = newChanIDs[:-channelsToCut or None]\n",
    "            \n",
    "            #Create pseudo channel locations\n",
    "            x = [0] * numChannels\n",
    "            y = list(range(1, numChannels+1))\n",
    "            locations = list(zip(x,y))\n",
    "            locations = np.array(locations)\n",
    "            locations.shape\n",
    "            newChanIDs = np.asarray(newChanIDs)\n",
    "            recordingObject.set_channel_locations(locations,newChanIDs)\n",
    "            #badChannelIDs, labels = si.detect_bad_channels(recording)\n",
    "            recordingObject = recordingObject.remove_channels(channelsToCut)\n",
    "            print(recordingObject)\n",
    "            print(recordingObject.get_channel_ids())\n",
    "            import re\n",
    "            channel_nums = recordingObject.get_channel_ids()\n",
    "            num_channels = recordingObject.get_num_channels()\n",
    "            channel_nums = channel_nums[0:totalTetrodeChannels].tolist()\n",
    "            for i in range(len(channel_nums)):\n",
    "                word = str(channel_nums[i])\n",
    "                word = re.findall(r'\\d+' , word)\n",
    "                channel_nums[i] = int(word[0]) - 1\n",
    "\n",
    "            #Generate tetrodes on a \"probe\"\n",
    "            probeGroup = ProbeGroup()\n",
    "            for i in range(int(totalTetrodeChannels/4)):\n",
    "                tetrode = generate_tetrode()\n",
    "                tetrode.move([i * 50, 0])\n",
    "                probeGroup.add_probe(tetrode)\n",
    "            probeGroup.set_global_device_channel_indices(channel_nums)\n",
    "            #probeGroup.set_global_device_channel_indices(np.arange(totalTetrodeChannels))\n",
    "            df = probeGroup.to_dataframe()\n",
    "            pd.set_option('display.max_columns', None)\n",
    "            pd.set_option('display.max_rows', None)\n",
    "            print(df)\n",
    "            #plot_probe_group(probeGroup, with_channel_index=True, same_axes=True)\n",
    "            #print(channel_nums)\n",
    "            #recording = pp.phase_shift(recording)\n",
    "            try:\n",
    "                recordingWProbe = recordingObject.set_probegroup(probeGroup,group_mode=\"by_probe\")\n",
    "                recordingCom = pp.common_reference(recordingWProbe, operator='median', reference=\"global\")\n",
    "                #print(recordingCom)\n",
    "                print(\"Channel Groups: \" + str(recordingCom.get_channel_groups()))\n",
    "                print(\"Channel IDs: \" + str(recordingCom.get_channel_ids()))\n",
    "                print(\"Channel Locations: \" + str(recordingCom.get_channel_locations()))\n",
    "            except Exception as e:\n",
    "                logging.error(f\"{e}\")\n",
    "                probeMismatch = True\n",
    "\n",
    "        elif (numberOfOrigChannels == 32):\n",
    "        #elif (numberOfOrigChannels == 32) or (numberOfOrigChannels == 35):\n",
    "            correctChannels = True\n",
    "            import numpy as np\n",
    "            #channel_ids = np.array(channel_ids)\n",
    "            print(recordingObject)\n",
    "            '''\n",
    "            try:\n",
    "                numChannels = 67\n",
    "                x = [0] * numChannels\n",
    "                y = list(range(1, numChannels+1))\n",
    "                locations = list(zip(x,y))\n",
    "                locations = np.array(locations)\n",
    "                locations.shape\n",
    "                origChanIDs = recordingObject.get_channel_ids().tolist()\n",
    "                auxChannels = ['AUX1','AUX2','AUX3','aux1','aux2','aux3']\n",
    "                auxChannels2 = ['AUX1','AUX2','AUX3']\n",
    "                newChanIDs = [channel for channel in origChanIDs if channel not in auxChannels]\n",
    "                newChanIDs = np.asarray(newChanIDs)\n",
    "                recordingObject.set_channel_locations(locations,origChanIDs)\n",
    "                recordingObject = recordingObject.remove_channels(auxChannels2)\n",
    "            '''\n",
    "            origChanIDs = recordingObject.get_channel_ids().tolist()\n",
    "            process = True\n",
    "            auxChannels = ['AUX1','AUX2','AUX3','aux1','aux2','aux3']\n",
    "            newChanIDs = [channel for channel in origChanIDs if channel not in auxChannels]\n",
    "            numChannels = len(newChanIDs)\n",
    "            totalTetrodeChannels = int(4 * math.floor(numChannels/4))\n",
    "            channelsToCut = newChanIDs[:numChannels-totalTetrodeChannels]\n",
    "            #newChanIDs = newChanIDs[:-channelsToCut or None]\n",
    "            x = [0] * numChannels\n",
    "            y = list(range(1, numChannels+1))\n",
    "            locations = list(zip(x,y))\n",
    "            locations = np.array(locations)\n",
    "            locations.shape\n",
    "            print(locations)\n",
    "            newChanIDs = np.asarray(newChanIDs)\n",
    "            recordingObject.set_channel_locations(locations,newChanIDs)\n",
    "            #badChannelIDs, labels = si.detect_bad_channels(recording)\n",
    "            recordingObject = recordingObject.remove_channels(channelsToCut)\n",
    "            print(recordingObject)\n",
    "            print(recordingObject.get_channel_ids())\n",
    "            import re\n",
    "            channel_nums = recordingObject.get_channel_ids()\n",
    "            num_channels = recordingObject.get_num_channels()\n",
    "            channel_nums = channel_nums[0:totalTetrodeChannels].tolist()\n",
    "            for i in range(len(channel_nums)):\n",
    "                word = str(channel_nums[i])\n",
    "                word = re.findall(r'\\d+' , word)\n",
    "                channel_nums[i] = int(word[0]) - 1\n",
    "\n",
    "            probeGroup = ProbeGroup()\n",
    "            for i in range(int(totalTetrodeChannels/4)):\n",
    "                tetrode = generate_tetrode()\n",
    "                tetrode.move([i * 50, 0])\n",
    "                probeGroup.add_probe(tetrode)\n",
    "            probeGroup.set_global_device_channel_indices(channel_nums)\n",
    "            #probeGroup.set_global_device_channel_indices(np.arange(totalTetrodeChannels))\n",
    "            df = probeGroup.to_dataframe()\n",
    "            pd.set_option('display.max_columns', None)\n",
    "            pd.set_option('display.max_rows', None)\n",
    "            print(df)\n",
    "            #plot_probe_group(probeGroup, with_channel_index=True, same_axes=True)\n",
    "            #print(channel_nums)\n",
    "            #recording = pp.phase_shift(recording)\n",
    "            try:\n",
    "                recordingWProbe = recordingObject.set_probegroup(probeGroup,group_mode=\"by_probe\")\n",
    "                recordingCom = pp.common_reference(recordingWProbe, operator='median', reference=\"global\")\n",
    "                #print(recordingCom)\n",
    "                print(\"Channel Groups: \" + str(recordingCom.get_channel_groups()))\n",
    "                print(\"Channel IDs: \" + str(recordingCom.get_channel_ids()))\n",
    "                print(\"Channel Locations: \" + str(recordingCom.get_channel_locations()))\n",
    "            except Exception as e:\n",
    "                logging.error(f\"{e}\")\n",
    "                probeMismatch = True\n",
    "        else:\n",
    "            correctChannels = False\n",
    "            logging.info(f\"Sorting Failed {recName} or Already Exists. Channels:{numberOfOrigChannels}\")\n",
    "            print(f\"Sorting Failed {recName} or Already Exists\")\n",
    "            print(f\"Number of channels is {numberOfOrigChannels}\")\n",
    "            #shutil.rmtree(dataCopyLoc)\n",
    "    else:\n",
    "        continue\n",
    "    if (correctChannels) and (not probeMismatch):\n",
    "        #thresholdList = [5.5]\n",
    "        #Sorts by group (each group is one tetrode)\n",
    "        for i in range(1):\n",
    "            sortingWorked = True\n",
    "            todayDate = date.today()\n",
    "            fullDir = Path(f\"{directory}\")\n",
    "            if not Path.is_dir(fullDir):\n",
    "                sortingMS5 = sorters.run_sorter_by_property(\n",
    "                    sorter_name =\"mountainsort5\", \n",
    "                    recording = recordingCom, \n",
    "                    grouping_property ='group',\n",
    "                    folder =  f\"{fullDir}/SortMS5\",\n",
    "                    #sorter parameters\n",
    "                    scheme = '3',\n",
    "                    #detect_threshold = thresholdList[i]\n",
    "                    #detect_sign = -1,\n",
    "                    #detect_time_radius_msec =  0.5,\n",
    "                    #snippet_T1 = 20,\n",
    "                    #snippet_T2 = 20,\n",
    "                    #npca_per_channel = 3,\n",
    "                    #npca_per_subdivision = 10,\n",
    "                    #snippet_mask_radius = 250,\n",
    "                    #scheme1_detect_channel_radius = 150,\n",
    "                    #scheme2_phase1_detect_channel_radius = 200,\n",
    "                    #scheme2_detect_channel_radius = 50,\n",
    "                    #scheme2_max_num_snippets_per_training_batch = 200,\n",
    "                    #scheme2_training_duration_sec = 300,\n",
    "                    #scheme2_training_recording_sampling_mode = 'uniform',\n",
    "                    #scheme3_block_duration_sec = 1800,\n",
    "                    #freq_min = 300,\n",
    "                    #freq_max = 6000,\n",
    "                    #filter = True,\n",
    "                    #whiten = True\n",
    "                )\n",
    "                toAddMS5 = ['MS5',sortingMS5]\n",
    "                sortingObjects.append(toAddMS5)\n",
    "                sortingT2 = sorters.run_sorter_by_property(\n",
    "                    sorter_name=\"tridesclous2\",\n",
    "                    recording = recordingCom,\n",
    "                    grouping_property = 'group',\n",
    "                    folder=f\"{fullDir}/SortT2\"\n",
    "                )\n",
    "                toaddT2 = ['T2',sortingT2]\n",
    "                sortingObjects.append(toaddT2)\n",
    "                if sortingWorked:\n",
    "                    with open(f\"{fullDir}/recording.pickle\" , 'wb') as f:\n",
    "                        pickle.dump(recordingCom,f)\n",
    "                    with open(f\"{fullDir}/sortingMS5.pickle\" , 'wb') as f:\n",
    "                        pickle.dump(sortingMS5,f)\n",
    "                    with open(f\"{fullDir}/sortingT2.pickle\" , 'wb') as f:\n",
    "                        pickle.dump(sortingT2,f)\n",
    "                    for sortingObj in sortingObjects:\n",
    "                        fullDir = f\"{fullDir}/{sortingObj[0]}\"\n",
    "                        try:\n",
    "                            cleaned = True\n",
    "                            #File path here used for GUI scripts\n",
    "                            #analyzerLoc = \"/home/moormanlab/Documents/OutputMS5Test/QualityMetricsTestFolder\"\n",
    "                            analyzerLoc = f\"{fullDir}/WaveformOut\"\n",
    "                            analyzerLocClean = f\"{fullDir}/WaveformOutClean\"\n",
    "                            analyzer = create_sorting_analyzer(recordingCom,sortingObj,analyzerLoc,overwrite=True)\n",
    "                            # TODO fixed bug\n",
    "                            amplitudes = spost.compute_spike_amplitudes(analyzer)\n",
    "                            amplitudes0 = amplitudes[0]\n",
    "                            thresh = -200\n",
    "                            mask = amplitudes0 >= thresh\n",
    "                            spikes = analyzer.sorting.to_spike_vector()\n",
    "                            largeAmpSpikes = spikes[mask]\n",
    "                            sortingLargeAmp = si.NumpySorting(\n",
    "                                spikes=largeAmpSpikes,\n",
    "                                sampling_frequency=analyzer.sampling_frequency,\n",
    "                                unit_ids=analyzer.unit_ids\n",
    "                                )\n",
    "                            # End TODO\n",
    "                            with open(f\"{fullDir}/sortingThresh.pickle\" , 'wb') as f:\n",
    "                                pickle.dump(sortingLargeAmp,f)\n",
    "                            cleanedAnalyzer = create_sorting_analyzer(recordingCom,sortingLargeAmp,analyzerLocClean,overwrite=True)\n",
    "                            compute_principal_components(cleanedAnalyzer)\n",
    "                            numSpikes = compute_num_spikes(cleanedAnalyzer)\n",
    "                            qualityMetrics = compute_quality_metrics(\n",
    "                                                waveform_extractor=cleanedAnalyzer,\n",
    "                                                metric_names=['num_spikes','firing_rate','snr','amplitude_cutoff','synchrony','firing_range','drift'],\n",
    "                                                qm_params=qual.get_default_qm_params()\n",
    "                                            )\n",
    "                            compute_spike_amplitudes(cleanedAnalyzer)\n",
    "                            compute_correlograms(cleanedAnalyzer)\n",
    "                            compute_template_similarity(cleanedAnalyzer)\n",
    "                            print(numSpikes)\n",
    "                            print(qualityMetrics)\n",
    "                        except:\n",
    "                            try:\n",
    "                                cleaned = False\n",
    "                                logging.error(f\"Cleaning Recording {recName} failed. Exporting Original Recording\")\n",
    "                                analyzerLoc = f\"{fullDir}/WaveformOut\"\n",
    "                                analyzerLocClean = f\"{fullDir}/WaveformOutClean\"\n",
    "                                analyzerExtractor = create_sorting_analyzer(recordingCom,sortingObj,analyzerLoc,overwrite=True)\n",
    "                                compute_principal_components(analyzerExtractor)\n",
    "                                numSpikes = compute_num_spikes(analyzerExtractor)\n",
    "                                qualityMetrics = compute_quality_metrics(\n",
    "                                                    waveform_extractor=analyzerExtractor,\n",
    "                                                    metric_names=['num_spikes','firing_rate','snr','amplitude_cutoff','synchrony','firing_range','drift'],\n",
    "                                                    qm_params=qual.get_default_qm_params()\n",
    "                                                )\n",
    "                                compute_spike_amplitudes(analyzerExtractor)\n",
    "                                compute_correlograms(analyzerExtractor)\n",
    "                                compute_template_similarity(analyzerExtractor)\n",
    "                                print(numSpikes)\n",
    "                                print(qualityMetrics)\n",
    "                            except Exception as e:\n",
    "                                pipelineBroke = True\n",
    "                                logging.error(e)\n",
    "                        if pipelineBroke:\n",
    "                            logging.error(f\"Pipeline broke for {recordingName}\")\n",
    "                        elif cleaned:\n",
    "                            try:\n",
    "                                export_report(cleanedAnalyzer,output_folder= f\"{fullDir}/GUIReport\",format=\"pdf\")\n",
    "                            except:\n",
    "                                logging.error(f\"Failed to export report to {fullDir}\")\n",
    "                                print(f\"Failed to export report to {fullDir}\")\n",
    "                            #try:\n",
    "                            #    export_to_phy(cleanedWe,output_folder= f\"{fullDir}/PhyReport\")\n",
    "                            #except:\n",
    "                            #    logging.error(f\"Failed to export phy to {fullDir}\")\n",
    "                            #    print(f\"Failed to export phy to {fullDir}\")\n",
    "                            export_to_phy(cleanedAnalyzer,output_folder= f\"{fullDir}/PhyReport\")\n",
    "                        elif not cleaned:\n",
    "                            try:\n",
    "                                export_report(analyzerExtractor,output_folder= f\"{fullDir}/GUIReport\",format=\"pdf\")\n",
    "                            except:\n",
    "                                logging.error(f\"Failed to export report to {fullDir}\")\n",
    "                                print(f\"Failed to export report to {fullDir}\")\n",
    "                            #try:\n",
    "                            #    export_to_phy(analyzerExtractor,output_folder= f\"{fullDir}/PhyReport\")\n",
    "                            #except:\n",
    "                            #    logging.error(f\"Failed to export phy to {fullDir}\")\n",
    "                            #    print(f\"Failed to export phy to {fullDir}\")\n",
    "                            export_to_phy(analyzerExtractor,output_folder= f\"{fullDir}/PhyReport\")\n",
    "                        #print(\"Deleting Data Locally\")\n",
    "                        #shutil.rmtree(dataCopyLoc)\n",
    "                        #print(\"Done Deleting\")\n",
    "                        #print(\"Moving Sorting to NAS\")\n",
    "                        if not pipelineBroke:\n",
    "                            try:\n",
    "                                shutil.move(\n",
    "                                    src=Path(f\"{fullDir}\"),\n",
    "                                    dst=Path(\"/mnt/Ephys/SortingOutputs/\")\n",
    "                                )\n",
    "                            except:\n",
    "                                logging.error(\"Output Directory Likely Exists\")\n",
    "                                print(\"Directory Likely Exists\")\n",
    "\n",
    "        else:\n",
    "            logging.error(f\"Sorting Failed {fullDir} or Already Exists\")\n",
    "            print(f\"Sorting Failed {fullDir} or Already Exists\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(channel_nums))\n",
    "#print(recordingObject.get_num_channels())\n",
    "#print(newChanIDs[:numChannels-totalTetrodeChannels])\n",
    "'''\n",
    "import math\n",
    "int(4 * math.floor(value/4))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "recname = \"CPWI17_2019-10-23_13-16-53_260uA LRRLLR\"\n",
    "dir = Path(f\"/home/moormanlab/Documents/CPWI17/Output{recName}\")\n",
    "dataSource = recording\n",
    "dataCopyLoc = Path(f\"/home/moormanlab/Documents/CPWI17/{recName}\")\n",
    "\n",
    "\n",
    "totalExperiments = len(next(os.walk(dataCopyLoc))[1])\n",
    "blockIndex = int(totalExperiments) - 1\n",
    "dataLocal = se.read_openephys(folder_path=dataCopyLoc, stream_id = '1', block_index = blockIndex)\n",
    "recordingObject = pp.bandpass_filter(dataLocal, freq_min=300, freq_max=6000)\n",
    "import numpy as np\n",
    "#channel_ids = np.array(channel_ids)\n",
    "print(recordingObject)\n",
    "numChannels = 64\n",
    "x = [0] * numChannels\n",
    "y = list(range(1, numChannels+1))\n",
    "locations = list(zip(x,y))\n",
    "locations = np.array(locations)\n",
    "locations.shape\n",
    "origChanIDs = recordingObject.get_channel_ids()  \n",
    "print(origChanIDs[0])\n",
    "print(type(list(origChanIDs)))\n",
    "'''\n",
    "#print(origChanIDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#File path here used for GUI scripts\n",
    "#waveformLoc2 = \"/home/moormanlab/Documents/OutputMS5Test/QualityMetricsTestFolder\"\n",
    "'''\n",
    "    import pickle\n",
    "    with open(fullDir + \"recording.pickle\" , 'wb') as f:\n",
    "        pickle.dump(recordingCom,f)\n",
    "        \n",
    "    with open(fullDir + \"sorting.pickle\" , 'wb') as f:\n",
    "        pickle.dump(sortingMS5,f)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "thresholds = [-150,150,-175,175,[-150,150],[-175,175]]\n",
    "for i in thresholds:\n",
    "    try: \n",
    "       len(i)\n",
    "    except:\n",
    "        thresh = i\n",
    "        single = True\n",
    "    else:\n",
    "        minthresh = i[0]\n",
    "        maxthresh = i[1]\n",
    "        single = False\n",
    "    waveformLoc2 = fullDir + '/' + \"WaveformOut\"\n",
    "    waveformLocClean = fullDir + '/' + \"WaveformOutClean\"\n",
    "    waveformExtractor2 = extract_waveforms(recordingCom,sortingMS5,waveformLoc2,overwrite=True)\n",
    "    amplitudes = spost.compute_spike_amplitudes(waveformExtractor2)\n",
    "    amplitudes0 = amplitudes[0]\n",
    "    if single:\n",
    "        mask = amplitudes0 >= thresh\n",
    "        threshSaveValue = thresh\n",
    "    if not single:\n",
    "        mask = (minthresh <= amplitudes0) & (amplitudes0 <= maxthresh)\n",
    "        absThresh = abs(minthresh)\n",
    "        threshSaveValue = f\"plusminus{absThresh}\"\n",
    "    spikes = waveformExtractor2.sorting.to_spike_vector()\n",
    "    largeAmpSpikes = spikes[mask]\n",
    "    \n",
    "    sortingLargeAmp = si.NumpySorting(\n",
    "        spikes=largeAmpSpikes,\n",
    "        sampling_frequency=waveformExtractor2.sampling_frequency,\n",
    "        unit_ids=waveformExtractor2.unit_ids\n",
    "        )\n",
    "    \n",
    "    cleanedWe = extract_waveforms(recordingCom,sortingLargeAmp,waveformLocClean,overwrite=True)\n",
    "    \n",
    "    compute_principal_components(cleanedWe)\n",
    "    numSpikes = compute_num_spikes(cleanedWe)\n",
    "    qualityMetrics = compute_quality_metrics(waveform_extractor=cleanedWe,\n",
    "                        metric_names=['num_spikes','firing_rate','snr','amplitude_cutoff','synchrony','firing_range','drift'],\n",
    "                        qm_params=qual.get_default_qm_params()\n",
    "    )\n",
    "    spost.compute_spike_locations(cleanedWe)\n",
    "    compute_spike_amplitudes(cleanedWe)\n",
    "    compute_correlograms(cleanedWe)\n",
    "    print(numSpikes)\n",
    "    print(qualityMetrics)\n",
    "    export_report(cleanedWe,output_folder= f\"{fullDir}{threshSaveValue}/GUIReport\")\n",
    "masktest = (-200 <= amplitudes0) & (amplitudes0 <= 200)\n",
    "print(amplitudes0)\n",
    "print(masktest)\n",
    "print(mask)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from spikeinterface.exporters import export_report, export_to_phy\n",
    "from spikeinterface import extract_waveforms\n",
    "\n",
    "import pickle\n",
    "def load(file):\n",
    "    FILENAME = file\n",
    "    with open(FILENAME,'rb') as f:\n",
    "        return pickle.load(f)\n",
    "#recordingCom = load(\"/home/moormanlab/Documents/OutputMS52024-01-10run0/recording.pickle\")\n",
    "#sortingMS5 = load(\"/home/moormanlab/Documents/OutputMS52024-01-10run0/sorting.pickle\")\n",
    "\n",
    "path = \"/home/moormanlab/Documents/OldOutputsorBackups/arbitrary1\"\n",
    "sortingLargeAmp = load(\"/home/moormanlab/Documents/OldOutputsorBackups/OutputCPWI17_2019-10-29_14-57-42_260uA RLLRRL/sortingThresh.pickle\")\n",
    "recordingCom = load(\"/home/moormanlab/Documents/OldOutputsorBackups/OutputCPWI17_2019-10-29_14-57-42_260uA RLLRRL/recording.pickle\")\n",
    "cleanedWe = extract_waveforms(recordingCom,sortingLargeAmp,path)\n",
    "export_to_phy(cleanedWe,output_folder= \"/home/moormanlab/Documents/OldOutputsorBackups/OutputCPWI17_2019-10-29_14-57-42_260uA RLLRRL/PhyReport\")\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import pickle\n",
    "def load(file):\n",
    "    FILENAME = file\n",
    "    with open(FILENAME,'rb') as f:\n",
    "        return pickle.load(f)\n",
    "#recordingCom = load(\"/home/moormanlab/Documents/OutputMS52024-01-10run0/recording.pickle\")\n",
    "#sortingMS5 = load(\"/home/moormanlab/Documents/OutputMS52024-01-10run0/sorting.pickle\")\n",
    "\n",
    "path = \"/home/moormanlab/Documents/OldOutputsorBackups/arbitrary2\"\n",
    "sortingLargeAmp = load(\"/home/moormanlab/Documents/OldOutputsorBackups/OutputCPWI17_2019-11-03_09-14-32_260uA LRRLLR/sortingThresh.pickle\")\n",
    "recordingCom = load(\"/home/moormanlab/Documents/OldOutputsorBackups/OutputCPWI17_2019-11-03_09-14-32_260uA LRRLLR/recording.pickle\")\n",
    "cleanedWe = extract_waveforms(recordingCom,sortingLargeAmp,path)\n",
    "export_to_phy(cleanedWe,output_folder= \"/home/moormanlab/Documents/OldOutputsorBackups/OutputCPWI17_2019-11-03_09-14-32_260uA LRRLLR/PhyReport\")\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from spikeinterface.postprocessing import compute_spike_amplitudes, compute_correlograms\n",
    "from spikeinterface.qualitymetrics import compute_quality_metrics, compute_num_spikes\n",
    "from spikeinterface.exporters import export_report\n",
    "import spikeinterface.qualitymetrics as qual\n",
    "from spikeinterface import extract_waveforms\n",
    "#File path here used for GUI scripts\n",
    "waveformLoc2 = \"/home/moormanlab/Documents/OutputMS5Test/QualityMetricsTestFolder\"\n",
    "#waveformLoc2 = fullDir + \"WaveformOut\"\n",
    "waveformExtractor2 = extract_waveforms(recordingCom,sortingMS5,waveformLoc2,overwrite=True)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from spikeinterface.postprocessing import compute_principal_components\n",
    "compute_principal_components(waveformExtractor2)\n",
    "numSpikes = compute_num_spikes(waveformExtractor2)\n",
    "qualityMetrics = compute_quality_metrics(waveform_extractor=waveformExtractor2,\n",
    "                        metric_names=['num_spikes','firing_rate','snr','amplitude_cutoff','synchrony','firing_range','drift'],\n",
    "                        qm_params=qual.get_default_qm_params()\n",
    ")\n",
    "compute_spike_amplitudes(waveformExtractor2)\n",
    "compute_correlograms(waveformExtractor2)\n",
    "\n",
    "print(numSpikes)\n",
    "print(qualityMetrics)\n",
    "\n",
    "export_report(waveformExtractor2,output_folder= fullDir + \"GUIReport\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import spikeinterface.exporters as exp\n",
    "#import spikeinterface.curation as cur\n",
    "#import spikeinterface.postprocessing as postp\n",
    "#import spikeinterface.qualitymetrics as qual\n",
    "##curatedSort = cur.threshold_min_num_spikes(sortingMS5, min_num_spike_threshold=100)\n",
    "#waveformLoc2 = \"/home/moormanlab/Documents/OutputMS5Test/WaveformOut\"\n",
    "#waveformExtractor2 = si.WaveformExtractor(recording,sortingMS5,waveformLoc2)\n",
    "##exp.export_to_phy(waveformExtractor2, output_folder=\"/home/moormanlab/Documents/OutputMS5Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import spikeinterface.full as si\n",
    "#si.get_default_sorter_params(\"mountainsort5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "print(recordingObject.get_channel_ids().tolist())\n",
    "test = recordingObject.get_channel_ids().tolist()\n",
    "print(test[-2]=='AUX2')\n",
    "newChanIDs = [channel for channel in origChanIDs if channel not in auxChannels]\n",
    "print(newChanIDs)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "origChanIDs = recordingObject.get_channel_ids().tolist()\n",
    "auxChannels = ['AUX1','AUX2','AUX3','aux1','aux2','aux3']\n",
    "newChanIDs = [channel for channel in origChanIDs if channel not in auxChannels]\n",
    "numChannels = len(newChanIDs)\n",
    "totalTetrodeChannels = int(4 * math.floor(numChannels/4))\n",
    "channelsToCut = newChanIDs[:numChannels-totalTetrodeChannels]\n",
    "x = [0] * numChannels\n",
    "y = list(range(1, numChannels+1))\n",
    "locations = list(zip(x,y))\n",
    "locations = np.array(locations)\n",
    "locations.shape\n",
    "newChanIDs = np.asarray(newChanIDs)\n",
    "recordingObject.set_channel_locations(locations,newChanIDs)\n",
    "recordingObject = recordingObject.remove_channels(channelsToCut)\n",
    "print(recordingObject)\n",
    "print(recordingObject.get_channel_ids())\n",
    "import re\n",
    "channel_nums = recordingObject.get_channel_ids()\n",
    "num_channels = recordingObject.get_num_channels()\n",
    "channel_nums = channel_nums[0:totalTetrodeChannels].tolist()\n",
    "for i in range(len(channel_nums)):\n",
    "    word = str(channel_nums[i])\n",
    "    word = re.findall(r'\\d+' , word)\n",
    "    channel_nums[i] = int(word[0]) - 1\n",
    "        \n",
    "\n",
    "probeGroup = ProbeGroup()\n",
    "for i in range(int(totalTetrodeChannels/4)):\n",
    "    tetrode = generate_tetrode()\n",
    "    tetrode.move([i * 50, 0])\n",
    "    probeGroup.add_probe(tetrode)\n",
    "probeGroup.set_global_device_channel_indices(channel_nums)\n",
    "\n",
    "print(channel_nums)\n",
    "\n",
    "recordingWProbe = recordingObject.set_probegroup(probeGroup,group_mode=\"by_probe\")\n",
    "'''\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f570c090a00da0b2b567fabc434ee2e78917831b15a3484213d7f0c1f9601010"
  },
  "kernelspec": {
   "display_name": "Python 3.11.5 ('spikeinterface')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
